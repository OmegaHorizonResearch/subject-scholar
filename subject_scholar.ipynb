{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Import Data for Train and Test\n",
    "\n",
    "Use a variety of topics and sources of text. Remove everything except letters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import corpus\n",
    "# nltk.download()\n",
    "\n",
    "\n",
    "# print(dir(corpus))\n",
    "# corp = corpus.gutenberg\n",
    "files = corpus.gutenberg.fileids()\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: This is only needed to open NLTK's downloads manager!\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text for CNN\n",
    "\n",
    "Each sample text needs to be transformed or encoded. Ideally, we want unrestricted vocabulary and minimum size. We can therefore encode individual letters and spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0                                Emma by Jane Austen\n",
      "1                                           VOLUME I\n",
      "2                                          CHAPTER I\n",
      "3  Emma Woodhouse handsome clever and rich with a...\n",
      "4  She was the youngest of the two daughters of a...\n",
      "5  Her mother had died too long ago for her to ha...\n",
      "6  Sixteen years had Miss Taylor been in Mr Woodh...\n",
      "7        Between it was more the intimacy of sisters\n",
      "8  Even before Miss Taylor had ceased to hold the...\n",
      "9  The real evils indeed of Emma s situation were...\n"
     ]
    }
   ],
   "source": [
    "# Get our source corpora from gutenberg in nltk.\n",
    "emma_sents = corpus.gutenberg.sents('austen-emma.txt')\n",
    "\n",
    "# Assign all of our samples that we'll be using.\n",
    "corpora = emma_sents[:2000]\n",
    "\n",
    "# Iterate across the sentences.\n",
    "alpha_sentences = pd.DataFrame()\n",
    "for sentence in corpora:\n",
    "    \n",
    "#     print(sent)\n",
    "    sent = list(filter(lambda x: str.isalpha(x), sentence))\n",
    "    sent = ' '.join(sent)\n",
    "    sent = pd.Series(sent)\n",
    "    alpha_sentences = alpha_sentences.append(sent, ignore_index=True)\n",
    "    \n",
    "print(alpha_sentences.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alpha_sentences.applymap(lambda x: print(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.'] Emma Woodhouse handsome clever and rich with a comfortable home and happy disposition seemed to unite some of the best blessings of existence and had lived nearly twenty one years in the world with very little to distress or vex her\n"
     ]
    }
   ],
   "source": [
    "print(emma_sents[3],alpha_sentences.iloc[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7752 <class 'nltk.corpus.reader.util.StreamBackedCorpusView'> ['_MAX_REPR_SIZE', '__add__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_cache', '_current_blocknum', '_current_toknum', '_encoding', '_eofpos', '_fileid', '_filepos', '_len', '_open', '_stream', '_toknum', 'close', 'count', 'fileid', 'index', 'iterate_from', 'read_block', 'unicode_repr']\n"
     ]
    }
   ],
   "source": [
    "print(len(emma_sents), type(emma_sents), dir(emma_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma by Jane Austen <class 'str'> ['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill'] ['Emma', 'by', 'Jane', 'Austen']\n"
     ]
    }
   ],
   "source": [
    "simple_string = alpha_sentences.iloc[0][0]\n",
    "print(simple_string, type(simple_string), dir(simple_string), simple_string.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(alpha_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0                                emma by jane austen\n",
      "1                                           volume i\n",
      "2                                          chapter i\n",
      "3  emma woodhouse handsome clever and rich with a...\n",
      "4  she was the youngest of the two daughters of a...\n"
     ]
    }
   ],
   "source": [
    "alpha_sentences_lower = alpha_sentences.applymap(lambda x: x.lower())\n",
    "print(alpha_sentences_lower.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'char_level', 'document_count', 'filters', 'fit_on_sequences', 'fit_on_texts', 'lower', 'num_words', 'sequences_to_matrix', 'split', 'texts_to_matrix', 'texts_to_sequences', 'texts_to_sequences_generator', 'word_counts', 'word_docs']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "Tknzr = text.Tokenizer(num_words=27,lower=True, char_level=True)\n",
    "print(dir(Tknzr))\n",
    "Tknzr.fit_on_texts(alpha_sentences_lower[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll need to vectorize the words, put that into a dataframe,\n",
    "char_vectors = Tknzr.texts_to_sequences(alpha_sentences_lower[0])\n",
    "\n",
    "char_vectors = alpha_sentences_lower.apply(lambda x: Tknzr.texts_to_sequences(x))\n",
    "# and then generate another dataframe that's a vector of letter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "26\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-40d30deca756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTknzr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "print(Tknzr.num_words)\n",
    "print(max(max(char_vectors[0])))\n",
    "print(min(min(char_vectors[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  [2, 13, 13, 4, 1, 20, 15, 1, 25, 4, 6, 2, 1, 4...\n",
      "1                       [22, 5, 12, 14, 13, 2, 1, 7]\n",
      "2                     [18, 8, 4, 21, 3, 2, 10, 1, 7]\n",
      "3  [2, 13, 13, 4, 1, 16, 5, 5, 11, 8, 5, 14, 9, 2...\n",
      "4  [9, 8, 2, 1, 16, 4, 9, 1, 3, 8, 2, 1, 15, 5, 1... <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(char_vectors.head(), type(char_vectors.iloc[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bin_vector(vect_size, position):\n",
    "    vector = np.zeros(vect_size)\n",
    "    vector[position] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Prep: Character Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_vectors = char_vectors.applymap(lambda x: x[:VECT_SIZE])\n",
    "padded_char_vects = char_vectors.applymap(lambda x: x+[1 for padding in range(VECT_SIZE-len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  [2, 13, 13, 4, 1, 20, 15, 1, 25, 4, 6, 2, 1, 4...\n",
      "1  [22, 5, 12, 14, 13, 2, 1, 7, 1, 1, 1, 1, 1, 1,...\n",
      "2  [18, 8, 4, 21, 3, 2, 10, 1, 7, 1, 1, 1, 1, 1, ...\n",
      "3  [2, 13, 13, 4, 1, 16, 5, 5, 11, 8, 5, 14, 9, 2...\n",
      "4  [9, 8, 2, 1, 16, 4, 9, 1, 3, 8, 2, 1, 15, 5, 1...\n",
      "[2, 13, 13, 4, 1, 20, 15, 1, 25, 4, 6, 2, 1, 4, 14, 9, 3, 2, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(padded_char_vects.head())\n",
    "print(padded_char_vects.iloc[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 512)\n"
     ]
    }
   ],
   "source": [
    "arr_char_vects = np.stack([np.stack(row) for row in padded_char_vects[0]])\n",
    "print(np.shape(arr_char_vects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Prep: Binary Char Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...\n",
      "4  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n"
     ]
    }
   ],
   "source": [
    "UNIQUE_CHARS = 27\n",
    "VECT_SIZE = 18\n",
    "\n",
    "# binary_vectors = char_vectors.applymap(lambda x:  x)\n",
    "\n",
    "binary_vectors = char_vectors.applymap(lambda x: [create_bin_vector(UNIQUE_CHARS, character-1) for character in x[-VECT_SIZE:]])\n",
    "\n",
    "print(binary_vectors.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "2       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "3       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...\n",
      "4       [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "5       [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "6       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "7       [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...\n",
      "8       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "9       [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "10      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...\n",
      "11      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...\n",
      "12      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...\n",
      "13      [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "14      [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "15      [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "16      [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "17      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...\n",
      "18      [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "19      [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "20      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "21      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "22      [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "23      [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "24      [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "25      [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "26      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...\n",
      "27      [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "28      [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "29      [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "                              ...                        \n",
      "1970    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1971    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1972    [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1973    [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1974    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1975    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1976    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1977    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...\n",
      "1978    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1979    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1980    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1981    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1982    [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1983    [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1984    [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1985    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1986    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1987    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1988    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1989    [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1990    [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1991    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1992    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...\n",
      "1993    [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1994    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1995    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...\n",
      "1996    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...\n",
      "1997    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1998    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...\n",
      "1999    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "Name: 0, Length: 2000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(binary_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(binary_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binary_vectors.applymap(lambda x: print(len(x), type(x), np.shape(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(['hi' for item in range(VECT_SIZE-len(binary_vectors.iloc[0][0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VECT_SIZE = 600\n",
    "padded_bin_vects = binary_vectors.applymap(lambda x: [np.zeros(UNIQUE_CHARS) for padding in range(VECT_SIZE-len(x))]+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    0\n",
       "14    0\n",
       "15    0\n",
       "16    0\n",
       "17    0\n",
       "18    0\n",
       "19    0\n",
       "20    0\n",
       "21    0\n",
       "22    0\n",
       "23    0\n",
       "24    0\n",
       "25    0\n",
       "26    0\n",
       "27    0\n",
       "28    0\n",
       "29    0\n",
       "...  ..\n",
       "1970  0\n",
       "1971  0\n",
       "1972  0\n",
       "1973  0\n",
       "1974  0\n",
       "1975  0\n",
       "1976  0\n",
       "1977  0\n",
       "1978  0\n",
       "1979  0\n",
       "1980  0\n",
       "1981  0\n",
       "1982  0\n",
       "1983  0\n",
       "1984  0\n",
       "1985  0\n",
       "1986  0\n",
       "1987  0\n",
       "1988  0\n",
       "1989  0\n",
       "1990  0\n",
       "1991  0\n",
       "1992  0\n",
       "1993  0\n",
       "1994  0\n",
       "1995  0\n",
       "1996  0\n",
       "1997  0\n",
       "1998  0\n",
       "1999  0\n",
       "\n",
       "[2000 rows x 1 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_bin_vects.applymap(lambda x: print(True) if len(x) > VECT_SIZE else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(padded_bin_vects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we were to parallelize the stacking, we'd have to first get a number of threads.\n",
    "# Then, we could split the original array into thread-number of sub-arrays.\n",
    "# We would form stacks for each sub-array,\n",
    "# and then join all the subarrays into a final stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 18, 27)\n"
     ]
    }
   ],
   "source": [
    "arr_bin_vects = np.stack([np.stack([np.stack(char) for char in row]) for row in padded_bin_vects[0]])\n",
    "print(np.shape(arr_bin_vects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(arr_bin_vects, test_size = 0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train the VAE-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Conv1D, Conv2D, Activation, MaxPooling2D, Dropout, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.constraints import unit_norm, min_max_norm\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.initializers import Orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 128  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(VECT_SIZE,UNIQUE_CHARS))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "# encoder_1 = Flatten()(input_img)\n",
    "reshape_1 = Reshape((1,VECT_SIZE,UNIQUE_CHARS))(input_img)\n",
    "#reconstruct letters from vectors\n",
    "cnn_1 = Conv2D(64,kernel_size=(2,UNIQUE_CHARS),strides=2,padding='same',activation='relu')(reshape_1)\n",
    "# mp_1 = MaxPooling2D(pool_size=(2,2), strides=2, padding='same')(cnn_1)\n",
    "dp_1 = Dropout(.01)(cnn_1)\n",
    "\n",
    "cnn_2 = Conv2D(128,kernel_size=(2, 64),strides=2,padding='same',activation='relu')(dp_1)\n",
    "# mp_2 = MaxPooling2D(pool_size=(2,2), strides=2, padding='same')(cnn_2)\n",
    "dp_2 = Dropout(.01)(cnn_2)\n",
    "\n",
    "cnn_3 = Conv2D(256,kernel_size=(2, 128),strides=2,padding='same',activation='relu')(dp_2)\n",
    "# mp_3 = MaxPooling2D(pool_size=(2,2), strides=2, padding='same')(cnn_3)\n",
    "dp_3 = Dropout(.01)(cnn_3)\n",
    "\n",
    "# cnn_4 = Conv2D(256,kernel_size=(2, 128),strides=2,padding='same',activation='relu')(dp_3)\n",
    "# # mp_4 = MaxPooling2D(pool_size=(2,2), strides=2, padding='same')(cnn_4)\n",
    "# dp_4 = Dropout(.03)(cnn_4)\n",
    "\n",
    "# cnn_5 = Conv2D(512,kernel_size=3,strides=3,padding='same',activation='relu')(cnn_4)\n",
    "# cnn_6 = Conv2D(2048,kernel_size=2,strides=2,padding='same',activation='relu')(cnn_5)\n",
    "# encode_reshape = Reshape((-1,1))(dp_4)\n",
    "# encoder_final_flat = Flatten()(cnn_4)\n",
    "# encoder_final_flat = Flatten()(encoder_flat)\n",
    "# kernel_constraint=min_max_norm(1, 3, 1, 0)\n",
    "encoded = Dense(encoding_dim, activation='softmax', kernel_constraint=unit_norm(1))(dp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "# decode_flat = Flatten()(encoded)\n",
    "decode_reshape = Reshape((1,1,-1))(encoded)\n",
    "# decode_cnn_1 = UpSampling2D(size=(2,2))(decode_reshape)\n",
    "# decode_cnn_2 = UpSampling2D(size=(3,3))(decode_cnn_1)\n",
    "# decode_cnn_3 = UpSampling2D(size=(4,1))(decode_cnn_2)\n",
    "# decode_cnn_4 = UpSampling2D(size=(5,1))(decode_cnn_3)\n",
    "\n",
    "# decode_1 = Dense(int(VECT_SIZE/8), activation='relu')(encoded)\n",
    "# decode_dp_1 = Dropout(0.03)(decode_1)\n",
    "\n",
    "# decode_2 = Dense(int(VECT_SIZE/4), activation='relu')(decode_dp_1)\n",
    "# decode_dp_2 = Dropout(0.03)(decode_2)\n",
    "\n",
    "# decode_3 = Dense(int(VECT_SIZE/2), activation='relu')(decode_dp_2)\n",
    "# decode_dp_3 = Dropout(0.03)(decode_3)\n",
    "\n",
    "# decode_4 = Dense(int(VECT_SIZE), activation='relu')(decode_dp_3)\n",
    "\n",
    "cnn_t_1 = Conv2DTranspose(3, kernel_size=(1,3),strides=(1,3),padding='valid',activation='relu')(decode_reshape)\n",
    "decode_dp_1 = Dropout(0.01)(cnn_t_1)\n",
    "\n",
    "cnn_t_2 = Conv2DTranspose(9, kernel_size=(1,3),strides=(1,3),padding='valid',activation='relu')(decode_dp_1)\n",
    "decode_dp_2 = Dropout(0.01)(cnn_t_2)\n",
    "\n",
    "cnn_t_3 = Conv2DTranspose(27,strides=(1,2),kernel_size=(1,2),padding='valid',activation='softmax')(decode_dp_2)\n",
    "decode_dp_3 = Dropout(0.01)(cnn_t_3)\n",
    "\n",
    "# Rework encoder with CNN to produce an output closer to the shape we want.\n",
    "# decode_3 = Dense(int(VECT_SIZE*UNIQUE_CHARS/5), activation='softmax')(cnn_t_2)\n",
    "\n",
    "# flat_decode = Flatten()(decode_cnn_4)\n",
    "decoded_4 = Reshape((VECT_SIZE,UNIQUE_CHARS))(decode_dp_3)\n",
    "# decoded_4 = Activation('softmax')(decoded_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_38 (InputLayer)        (None, 18, 27)            0         \n",
      "_________________________________________________________________\n",
      "reshape_93 (Reshape)         (None, 1, 18, 27)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 1, 9, 64)          93376     \n",
      "_________________________________________________________________\n",
      "dropout_156 (Dropout)        (None, 1, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 1, 5, 128)         1048704   \n",
      "_________________________________________________________________\n",
      "dropout_157 (Dropout)        (None, 1, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 1, 3, 256)         8388864   \n",
      "_________________________________________________________________\n",
      "dropout_158 (Dropout)        (None, 1, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1, 3, 128)         32896     \n",
      "_________________________________________________________________\n",
      "reshape_94 (Reshape)         (None, 1, 1, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_97 (Conv2DT (None, 1, 3, 3)           3459      \n",
      "_________________________________________________________________\n",
      "dropout_159 (Dropout)        (None, 1, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_98 (Conv2DT (None, 1, 9, 9)           90        \n",
      "_________________________________________________________________\n",
      "dropout_160 (Dropout)        (None, 1, 9, 9)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_99 (Conv2DT (None, 1, 18, 27)         513       \n",
      "_________________________________________________________________\n",
      "dropout_161 (Dropout)        (None, 1, 18, 27)         0         \n",
      "_________________________________________________________________\n",
      "reshape_95 (Reshape)         (None, 18, 27)            0         \n",
      "=================================================================\n",
      "Total params: 9,567,902\n",
      "Trainable params: 9,567,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_inbound_node', '_built', '_feed_input_names', '_feed_input_shapes', '_feed_inputs', '_fit_loop', '_get_deduped_metrics_names', '_get_node_attribute_at_index', '_make_predict_function', '_make_test_function', '_make_train_function', '_output_mask_cache', '_output_shape_cache', '_output_tensor_cache', '_per_input_losses', '_per_input_updates', '_predict_loop', '_standardize_user_data', '_test_loop', '_updated_config', 'add_loss', 'add_update', 'add_weight', 'assert_input_compatibility', 'build', 'built', 'call', 'compile', 'compute_mask', 'compute_output_shape', 'constraints', 'container_nodes', 'count_params', 'evaluate', 'evaluate_generator', 'fit', 'fit_generator', 'from_config', 'get_config', 'get_input_at', 'get_input_mask_at', 'get_input_shape_at', 'get_layer', 'get_losses_for', 'get_output_at', 'get_output_mask_at', 'get_output_shape_at', 'get_updates_for', 'get_weights', 'inbound_nodes', 'input', 'input_layers', 'input_layers_node_indices', 'input_layers_tensor_indices', 'input_mask', 'input_names', 'input_shape', 'input_spec', 'inputs', 'internal_input_shapes', 'internal_output_shapes', 'layers', 'layers_by_depth', 'load_weights', 'losses', 'name', 'nodes_by_depth', 'non_trainable_weights', 'outbound_nodes', 'output', 'output_layers', 'output_layers_node_indices', 'output_layers_tensor_indices', 'output_mask', 'output_names', 'output_shape', 'outputs', 'predict', 'predict_generator', 'predict_on_batch', 'reset_states', 'run_internal_graph', 'save', 'save_weights', 'set_weights', 'state_updates', 'stateful', 'summary', 'supports_masking', 'test_on_batch', 'to_json', 'to_yaml', 'train_on_batch', 'trainable', 'trainable_weights', 'updates', 'uses_learning_phase', 'weights']\n",
      "[(None, 18, 27)]\n"
     ]
    }
   ],
   "source": [
    "print(dir(autoencoder))\n",
    "# print(autoencoder._feed_output_shapes)\n",
    "print(autoencoder.internal_output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(1,3,encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer_0 = autoencoder.layers[-8](encoded_input)\n",
    "decoder_layer_1 = autoencoder.layers[-7](decoder_layer_0)\n",
    "decoder_layer_2 = autoencoder.layers[-6](decoder_layer_1)\n",
    "decoder_layer_3 = autoencoder.layers[-5](decoder_layer_2)\n",
    "decoder_layer_4 = autoencoder.layers[-4](decoder_layer_3)\n",
    "decoder_layer_5 = autoencoder.layers[-3](decoder_layer_4)\n",
    "decoder_layer_6 = autoencoder.layers[-2](decoder_layer_5)\n",
    "decoder_layer_7 = autoencoder.layers[-1](decoder_layer_6)\n",
    "\n",
    "\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the autoencoder.\n",
    "autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 35s - loss: 3.2606 - val_loss: 3.1447\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 34s - loss: 3.2462 - val_loss: 3.1244\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 33s - loss: 3.2159 - val_loss: 3.1000\n",
      "Epoch 4/15\n",
      "1000/1000 [==============================] - 34s - loss: 3.1887 - val_loss: 3.0663\n",
      "Epoch 5/15\n",
      "1000/1000 [==============================] - 34s - loss: 3.1475 - val_loss: 3.0199\n",
      "Epoch 6/15\n",
      "1000/1000 [==============================] - 34s - loss: 3.1079 - val_loss: 2.9634\n",
      "Epoch 7/15\n",
      "1000/1000 [==============================] - 34s - loss: 3.0460 - val_loss: 2.9027\n",
      "Epoch 8/15\n",
      " 128/1000 [==>...........................] - ETA: 29s - loss: 3.0388"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-618adac2f59f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 validation_data=(X_test, X_test))\n\u001b[0m",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the autoencoder.\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=15,\n",
    "                batch_size=64,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(X_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "   [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "   [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "   [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "   [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[  3.88820176e-18   2.62669973e-16   3.67117645e-19 ...,\n",
      "      2.25916803e-19   1.03456819e-17   4.65497798e-15]\n",
      "   [  8.02864990e-13   3.22338556e-09   6.01250733e-12 ...,\n",
      "      2.36880404e-14   3.45205852e-13   1.04678294e-10]\n",
      "   [  6.22427179e-16   1.16672731e-13   1.26295580e-13 ...,\n",
      "      7.73524318e-17   7.11743828e-14   4.93989234e-15]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "   [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "   [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "   [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "   [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "   [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "   [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,\n",
      "      0.00000000e+00   0.00000000e+00   0.00000000e+00]]]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.06924267  0.06346015  0.05739995 ...,  0.02252641  0.0268419\n",
      "    0.02736868]\n",
      "  [ 0.06769636  0.06353331  0.05270862 ...,  0.02701296  0.0227058\n",
      "    0.01872784]\n",
      "  [ 0.0805166   0.06656609  0.07711747 ...,  0.01772137  0.02189776\n",
      "    0.02753279]\n",
      "  ..., \n",
      "  [ 0.06963973  0.06064435  0.05357018 ...,  0.02365584  0.01972856\n",
      "    0.01481067]\n",
      "  [ 0.09090226  0.07575576  0.06563896 ...,  0.01639288  0.02258601\n",
      "    0.01905673]\n",
      "  [ 0.06458323  0.06968544  0.07496871 ...,  0.02401928  0.01833084\n",
      "    0.01400757]]\n",
      "\n",
      " [[ 0.06924267  0.06346015  0.05739995 ...,  0.02252641  0.0268419\n",
      "    0.02736868]\n",
      "  [ 0.06769636  0.06353331  0.05270862 ...,  0.02701296  0.0227058\n",
      "    0.01872784]\n",
      "  [ 0.0805166   0.06656609  0.07711747 ...,  0.01772137  0.02189776\n",
      "    0.02753279]\n",
      "  ..., \n",
      "  [ 0.06963973  0.06064435  0.05357018 ...,  0.02365584  0.01972856\n",
      "    0.01481067]\n",
      "  [ 0.09090226  0.07575576  0.06563896 ...,  0.01639288  0.02258601\n",
      "    0.01905673]\n",
      "  [ 0.06458323  0.06968544  0.07496871 ...,  0.02401928  0.01833084\n",
      "    0.01400757]]\n",
      "\n",
      " [[ 0.06923464  0.06345387  0.05739595 ...,  0.02252929  0.02684392\n",
      "    0.02737024]\n",
      "  [ 0.06768791  0.0635264   0.05270715 ...,  0.02701522  0.02270861\n",
      "    0.01873103]\n",
      "  [ 0.08050403  0.06656098  0.07710584 ...,  0.01772523  0.02190116\n",
      "    0.02753488]\n",
      "  ..., \n",
      "  [ 0.06963173  0.06063967  0.0535678  ...,  0.02365876  0.01973253\n",
      "    0.01481508]\n",
      "  [ 0.0908836   0.07574518  0.06563329 ...,  0.01639711  0.02258945\n",
      "    0.0190613 ]\n",
      "  [ 0.06457815  0.06967623  0.07495543 ...,  0.02402217  0.01833495\n",
      "    0.01401194]]\n",
      "\n",
      " ..., \n",
      " [[ 0.06924267  0.06346015  0.05739995 ...,  0.02252641  0.0268419\n",
      "    0.02736868]\n",
      "  [ 0.06769636  0.06353331  0.05270862 ...,  0.02701296  0.0227058\n",
      "    0.01872784]\n",
      "  [ 0.0805166   0.06656609  0.07711747 ...,  0.01772137  0.02189776\n",
      "    0.02753279]\n",
      "  ..., \n",
      "  [ 0.06963973  0.06064435  0.05357018 ...,  0.02365584  0.01972856\n",
      "    0.01481067]\n",
      "  [ 0.09090226  0.07575576  0.06563896 ...,  0.01639288  0.02258601\n",
      "    0.01905673]\n",
      "  [ 0.06458323  0.06968544  0.07496871 ...,  0.02401928  0.01833084\n",
      "    0.01400757]]\n",
      "\n",
      " [[ 0.06924267  0.06346015  0.05739995 ...,  0.02252641  0.0268419\n",
      "    0.02736868]\n",
      "  [ 0.06769636  0.06353331  0.05270862 ...,  0.02701296  0.0227058\n",
      "    0.01872784]\n",
      "  [ 0.0805166   0.06656609  0.07711747 ...,  0.01772137  0.02189776\n",
      "    0.02753279]\n",
      "  ..., \n",
      "  [ 0.06963973  0.06064435  0.05357018 ...,  0.02365584  0.01972856\n",
      "    0.01481067]\n",
      "  [ 0.09090226  0.07575576  0.06563896 ...,  0.01639288  0.02258601\n",
      "    0.01905673]\n",
      "  [ 0.06458323  0.06968544  0.07496871 ...,  0.02401928  0.01833084\n",
      "    0.01400757]]\n",
      "\n",
      " [[ 0.06924267  0.06346015  0.05739995 ...,  0.02252641  0.0268419\n",
      "    0.02736868]\n",
      "  [ 0.06769636  0.06353331  0.05270862 ...,  0.02701296  0.0227058\n",
      "    0.01872784]\n",
      "  [ 0.0805166   0.06656609  0.07711747 ...,  0.01772137  0.02189776\n",
      "    0.02753279]\n",
      "  ..., \n",
      "  [ 0.06963973  0.06064435  0.05357018 ...,  0.02365584  0.01972856\n",
      "    0.01481067]\n",
      "  [ 0.09090226  0.07575576  0.06563896 ...,  0.01639288  0.02258601\n",
      "    0.01905673]\n",
      "  [ 0.06458323  0.06968544  0.07496871 ...,  0.02401928  0.01833084\n",
      "    0.01400757]]]\n"
     ]
    }
   ],
   "source": [
    "print(decoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 18, 27)\n"
     ]
    }
   ],
   "source": [
    "# print(dir(cnn_1))\n",
    "print(np.shape(decoded_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'char_level', 'document_count', 'filters', 'fit_on_sequences', 'fit_on_texts', 'index_docs', 'lower', 'num_words', 'sequences_to_matrix', 'split', 'texts_to_matrix', 'texts_to_sequences', 'texts_to_sequences_generator', 'word_counts', 'word_docs', 'word_index']\n"
     ]
    }
   ],
   "source": [
    "print(dir(Tknzr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, 'e': 2, 't': 3, 'a': 4, 'o': 5, 'n': 6, 'i': 7, 'h': 8, 's': 9, 'r': 10, 'd': 11, 'l': 12, 'm': 13, 'u': 14, 'y': 15, 'w': 16, 'f': 17, 'c': 18, 'g': 19, 'b': 20, 'p': 21, 'v': 22, 'k': 23, 'x': 24, 'j': 25, 'q': 26, 'z': 27} [' ', 'e', 't', 'a', 'o', 'n', 'i', 'h', 's', 'r', 'd', 'l', 'm', 'u', 'y', 'w', 'f', 'c', 'g', 'b', 'p', 'v', 'k', 'x', 'j', 'q', 'z'] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n"
     ]
    }
   ],
   "source": [
    "print(Tknzr.word_index, list(Tknzr.word_index.keys()),list(Tknzr.word_index.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Binary Character Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   o t   o o   o t\n",
      "   o t   o o   o t\n",
      "   o t   o o   o t\n",
      "   o t   o o   o t\n",
      "   o t   o o   o t\n",
      "   o t   o o   o t\n",
      "severe cold indeed\n",
      "pectable young man\n",
      "                oh\n",
      " have improved her\n",
      "acities or neither\n",
      "    leave it to me\n"
     ]
    }
   ],
   "source": [
    "# for a decoded image, we need to go over each sentence.\n",
    "# find the position of the highest value in the char vector.\n",
    "# find the character its position corresponds to.\n",
    "# then each character, and reconstruct the sentence.\n",
    "for sentence in decoded_imgs[:6]:\n",
    "    sentence_letters = []\n",
    "    for char_vector in sentence:\n",
    "        highest_char = np.argmax(char_vector)\n",
    "#         print(highest_char)\n",
    "        position = highest_char + 1\n",
    "        letter = list(Tknzr.word_index.keys())[list(Tknzr.word_index.values()).index(position)]\n",
    "        sentence_letters.append(letter)\n",
    "    print(''.join(sentence_letters))\n",
    "# Tknzr.word_index\n",
    " \n",
    "for sentence in X_test[:6]:\n",
    "    sentence_letters = []\n",
    "    for char_vector in sentence:\n",
    "#         highest_char = np.round(char_vector)\n",
    "#         print(highest_char)\n",
    "        position = np.argmax(char_vector)+1\n",
    "        letter = list(Tknzr.word_index.keys())[list(Tknzr.word_index.values()).index(position)]\n",
    "        sentence_letters.append(letter)\n",
    "    print(''.join(sentence_letters))\n",
    "# [print(sentence) for sentence in X_test[:6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Integer Character Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sioniiiininnnnnnnnnnnnnnnnnnnonooooooooaoaaoaaaaaaatattttttttttttttttttttttttttttttttttttttttttttt tttttttttttttttttttteeeeeteeeeeeteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeee eee eeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeee  eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee  eeee  eee eeeee\n",
      "hnaonnnnonooooonoooooooooooooaoooaoaaaaaaaaaaattttattttttttetteeeteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeee eee eeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeee  eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee  eeee  eee eeeee\n",
      "ioaooooooooooooooooaoaaaooaaaaaaaaaaaaatattatttttttttttteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeee eee eeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeee  eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee  eeee  eee eeeee\n",
      "hnonnnnnonnnnnnnnnnooooonooooooooooooaaaaaaaaatttaattttttttttttttteeetteeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeee eee eeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeee  eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee  eeee  eee eeeee\n",
      "hnaonnnnonoooooooooooooooooooaoooaoaaaaaaaaaaattttatttttttteteeeeteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeee eee eeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeee  eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee  eeee  eee eeeee\n",
      "ioaooooooooooooooooooaooooaoaaaaaaaaaaatattattttttttttttteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeee eee eeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeee  eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee  eeee  eee eeeee\n",
      "emma smiled and answered my visit was of use to the nervous part of her complaint i hope but not even i can charm away a sore throat it is a most severe cold indeed                                                                                                                                                                                                                                                                                                                                                            \n",
      "i have no doubt of his being a very respectable young man                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "oh                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
      "come said he you are anxious for a compliment so i will tell you that you have improved her                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "a poet in love must be encouraged in both capacities or neither                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
      "leave it to me                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n"
     ]
    }
   ],
   "source": [
    "# for a decoded image, we need to go over each sentence.\n",
    "# find the position of the highest value in the char vector.\n",
    "# find the character its position corresponds to.\n",
    "# then each character, and reconstruct the sentence.\n",
    "for sentence in decoded_imgs[:6]:\n",
    "    sentence_letters = []\n",
    "    for char_vector in sentence:\n",
    "        highest_char = np.round(char_vector)\n",
    "#         print(highest_char)\n",
    "        position = highest_char + 1\n",
    "        letter = list(Tknzr.word_index.keys())[list(Tknzr.word_index.values()).index(position)]\n",
    "        sentence_letters.append(letter)\n",
    "    print(''.join(sentence_letters))\n",
    "    \n",
    "    \n",
    "for sentence in X_test[:6]:\n",
    "    sentence_letters = []\n",
    "    for char_vector in sentence:\n",
    "#         highest_char = np.round(char_vector)\n",
    "#         print(highest_char)\n",
    "        position = char_vector \n",
    "        letter = list(Tknzr.word_index.keys())[list(Tknzr.word_index.values()).index(position)]\n",
    "        sentence_letters.append(letter)\n",
    "    print(''.join(sentence_letters))\n",
    "# [print(sentence) for sentence in X_test[:6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Our CNN\n",
    "\n",
    "We need to see how well our CNN can match the labels applied through a more conventional method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Samples\n",
    "\n",
    "Apply topic labels to each sample using a conventional technique like SVD. Later, this might be expanded to include selection among assignment alternatives based on information criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-fe5d71d02a82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memma_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \"\"\"\n\u001b[1;32m    498\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_non_neg_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LatentDirichletAllocation.fit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py\u001b[0m in \u001b[0;36m_check_non_neg_array\u001b[0;34m(self, X, whom)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \"\"\"\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/brian/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;31m# make sure we actually converted to numeric:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"O\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "lda = LDA()\n",
    "lda.fit(emma_sents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
